# Greedy

Un algoritmo greedy es un algoritmo que va reduciendo el problema en problemas más pequeños, escogiendo la mejor opción en cada momento. Es como fuerza bruta pero **nunca hace backtracking**.

### Knapsack problem

Hay que meter `n` objetos en una mochila, estos objetos tienen un valor `v(i)` y un peso `w(i)`. Queremos maximizar `sum(v(i))` sin pasarse del peso máximo que puede tener la mochila `sum(w(i)) <= W`.

#### Algoritmo greedy

```py
def knapsack(mochila, objetos, v, w):
	sort(objetos)	# ordenar objetos según densidad (v(i) / w(i)) decreciente

	while not objetos.empty() and mochila.peso + objetos[0].peso <= mochila.peso_maximo:
		mochile.peso += objetos[0].peso
		mochila.add(objetos[0])
		objetos.erase(objetos[0])
```

> Obviamente, este algoritmo no garantiza el óptimo, y puede dar soluciones subóptimas, sin embargo, su coste es `O(n + n * log(n))`. Y por tanto es bastante eficiente.

### Activity selection problem

Te dan `n` actividades que se tienen que procesar desde `starts(i)` hasta `ends(i)`. Sin embargo, hay algunas actividades que se solapan. Se pide escoger el máximo número de actividades que tal que no se solapen.

#### Algoritmo greedy

```py
def activity_selection(A, starts, ends):
	# ordenar actividades según ends(i) ascendente
	sort(A)

	# Init
	procesadas += A[0]

	for i in 1..(n-1):
		if starts(A[i]) >= ends(procesadas.last):
			procesadas += A[i]

	return procesadas
```

> Este sí que es óptimo, aunque la mayoría de greedy no lo son.

### Weighted activity selection problem

Es el mismo problema que el anterior, pero en este caso queremos maximizar la suma de los pesos `w(i)` de cada actividad, no el número de actividades procesadas.

#### Algoritmo greedy

```py
def overlaps(a1, a2):
	return (ends(a1) <= ends(a2) and ends(a1) >= starts(a2)) or
		   (ends(a2) <= ends(a1) and ends(a2) >= starts(a1))

def activity_selection(A, w, starts, ends):
	# ordenar actividades según w(i) descendente
	sort(A)

	# Init
	procesadas += A[0]

	while not A.empty():
		# Add activity
		procesadas += A.first

		# Delete overlapping
		for a in A:
			if overlaps(a, procesadas.last):
				A.erase(a)

	return procesadas
```

> Este no es óptimo ya que podemos coger una actividad de gran peso que se solape con muchas de peso menor pero las cuales la suma de sus pesos sería superior

### Minimize lateness

Igual que los anteriores, tienes unas actividades con un duracion y un tiempo limite en el que tiene que haber estado ejecutada, pero ahora, el objetivo es minimizar el tiempo que pasamos inactivos desde que procesamos una actividad hasta que procesamos otra. Es decir, minimizar el tamaño de los huecos, intentar hacer todas las actividades seguidas.

#### Algoritmo greedy

```py
def minimize_lateness(A, duration, finished_by):
	sort(A)	# ordenar segun finished_by(A[i]) ascendente

	t = 0
	for i in 0..(n-1):
		process_from_to(A[i], t, t + duration(A[i]))	# procesar actividad A[i] desde t hasta t + duration(i)
		t += duration(A[i])	# update t
```

> Con esto consigues hacer aquellas actividades que sean más urgentes antes que las que no lo son. La complejidad es de `O(n * log(n))` (ordenar).

### Minimum Spanning Tree (MST)

Construir un árbol a partir de un grafo tal que conecte todos los vértices y que además la suma de sus aristas sea mínima.

### Union-Find

Es una estructura de datos capaz de mantener una lista dinámica de conjuntos disjuntos. Puedes realizar las siguientes operaciones:
- **MAKESET(x)**: Crea un conjunto con `x` dentro. → `Θ(1)`
- **UNION(x, y)**: Añade el conjunto donde se encuentra `y` al final del conjunto donde se encuentra `x`. →  `Θ(log(n))`
- **FIND(x)**: Devuelve el representante del conjunto donde se encuentra `x`. → `Θ(log(n))`

#### Prim

```py
def prim(G, w):
	# Init MST
	T = []

	for v in V:
		for e in E.sorted_by_weight():
			if intersection(T, e) != [] and not forma_ciclo(T + e):
				T.add(e)
```

Su complejidad varía dependiendo de la implementación de la priority queue.
- Si es una **array** sin ordenar, es `O(n^2)`.
- Si es un **heap**, es `O(m * log(n))`.
- Si es un **fibonacci heap**, es `O(m + n * log(n))`.

#### Kruskal

```py
def kruskal(G, w):
	sort(E)	# Sort edges by w(E[i]) increasing

	for v in V:
		for e in E:
			if not forma_ciclo(T + e):
				T.add(e)
```

Tiene una complejidad de `O(m * log(m))` (de ordenar), pero como `m <= n^2`. Entonces `O(m * log(n))`.

También se puede implementar con un union-find:

```py
def kruskal_union_find(G, w):
	sort(E)	# Sort edges by w(E[i]) increasing

	T = []	# empty mst

	for v in V:
		makeset(v)

	for e in E:
		(u, v) = e.edges
		if find(v) != find(u):	# no hay interseccion
			T += e
			union(u, v)

	return T
```

> Las operaciones de union-find tardan `O(m * log(n))` (que se puede amortizar como `O(m)` ya que `log(n)` es casi constante) pero ordenar tarda `O(n * log(n))`.

### Graph connectivity

Te dan un grafo `G` y dados dos vértices, decir si pertenecen a la misma componente conexa.

```py
def same_connected_component(G, v1, v2):
	for vert in V:
		makeset(vert)

	for (u, v) in E:
		if find(u) != find(v):
			union(u, v)

	return find(v1) == find(v2)
```

### Data compression

Comprimir un archivo sin perder calidad usando un árbol de prefijos. Es decir, haciendo que aquellos "términos" de un diccionario definido por nosotros que se repitan más se representen con menos bits que aquellos que tengan una frecuencia menor.

Para calcular la frecuencia usamos:

<p align="center">
	<img src="https://latex.codecogs.com/svg.latex?frec%28x%29%20%3D%20%5Cfrac%20%7Bocurrences%28x%29%29%7D%7Bn%7D" width=35%>
</p>

Dados los prefijos de los términos del diccionario podemos calcular la longitud de la codificación así:

<p align="center">
	<img src="https://latex.codecogs.com/svg.latex?longitud%20%3D%20n%5C%20%5Ctimes%5C%20%5Csum%20%7B%5CBig%28%5Cmathit%7Bfrec%7D%28x%29%20%5Ctimes%20%5Cmathit%7Bbits%5C_prefijo%7D%28x%29%5CBig%29%7D" width=45%>
</p>

Y podemos saber el número promedio de bits que requiere cada símbolo, así:

<p align="center">
	<img src="https://latex.codecogs.com/svg.latex?promedio%5C_bits%20%3D%20%5Csum%20%7B%5CBig%28%5Cmathit%7Bfrec%7D%28x%29%20%5Ctimes%20%5Cmathit%7Bbits%5C_prefijo%7D%28x%29%5CBig%29%7D" width=45%>
</p>

#### Ejemplo

La idea es hacer un árbol binario con tantas hojas como términos tenga el diccionario. Una vez hecho, comenzar por el que tenga menos frecuencia y asignarlo a la rama más profunda, continuar con el siguiente... hasta llegar al más cercano.



> El problema es que no siempre será el óptimo. Si queremos el óptimo, habrá que usar Huffman (siguiente punto).

```py
diccionario = [ a,   b,   c,   d,   e ]
frecuencias = [.32, .25, .20, .18, .05]

prefijo(a) = 0b11
prefijo(b) = 0b01
prefijo(c) = 0b001
prefijo(d) = 0b10
prefijo(e) = 0b000

# Para saber su compresion hacemos:
compresion = 0.0
for termino in diccionario:
	compresion += frecuencia(termino) * prefijo(termino).bits
```

#### Huffman

Si queremos conseguir el árbol óptimo, usamos el algoritmo greedy de Huffman.

```py
def huffman(diccionario, frecuencias):

	T = []	# empty tree
	Q = []	# priority queue que ordena por frecuencia(x) ascendente

	for termino in diccionario:
		Q.push(termino)

	while not Q.empty():
		T.make_node(z)	# create new node z
		x = Q.pop()
		y = Q.pop()
		T[z].sons(x, y)	# make x and y sons of z
		frecuencia(z) = frecuencia(x) + frecuencia(y)
		Q.push(z)
```

> Si la priority queue está implementada con un heap, el algoritmo tiene complejidad `O(n * log(n))`.
